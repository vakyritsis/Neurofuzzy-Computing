# Neurofuzzy Computing Projects
Welcome to the repository for the Neurofuzzy Computing course (ECE447) of Uth. The subject of the course covers the area of ​​neural networks with reference to other techniques from the broader area of ​​computational intelligence, such as fuzzy systems.
## Course context 
The course delves into fundamental concepts and operations of neural networks (NNs) and fuzzy logic. It focuses on various types of neural networks, optimization methods for training deep neural networks, deep architectures of NNs, distributed learning, and fuzzy logic theories.

## Topics Covered
The course covers the following topics:
- Basic concepts of neural network design and operation
- Introduction to fuzzy logic and fuzzy subset theory
- Various types of neural networks, including:
- Feedforward NNs:
- MultiLayer Perceptrons (MLPs)
- Convolutional NNs (CNNs)
- Radial Basis Function NNs
- Competitive NNs:
- Self-Organizing Map (SOM)
- Learning Vector Quantization (LVQ)
- Hebbian NNs:
- Linear Autoencoder
- Recurrent NNs:
- Layered Digital Dynamic Networks (LDDN)
- Long-Short Term Memory (LSTM)
- Gated Recurrent Units (GRUs)
- Optimization methods for training deep neural networks:
  - (Stochastic) Gradient Descent
  - Adagrad
  - Adadelta
  - Adam
  - Newton
  - Conjugate Gradient
  - BFGS
- Deep architectures of NNs:
  - MLPs
  - CNNs
  - RNNs
  - Transformers
- Distributed learning:
  - Purely Distributed
  - Hybrid Federated Deep Learning

## Repository Contents
During the semester we were asked to face some series of problems for the understanding and application of the theoretical background. Also through the project we developed our own neural network for hierarchical text classification. Each folder includes the problems statements, the answers in latex report and the source code.

1. Problem set 1 introduce us to Neural Networks and to the Fuzzy Logic. We get to understand mathematics  that is necessary for Deep Learning, create our first Neural Networks and work with ADALINE NNs.
2. In Problem set 2 we learn about second-order minimizers, MLP and backpropagation, modern optimizers and CNNs.
3. Problem set 3 comes to complete our journey with Neural Networkds with Radial Basis Function NNs,  Hebbian and Competitive learning, RNNs and with some extra problems on Fuzzy Logic
4. The Coding Projects was about creating our own Neural Network for hierarchical text classification. We were given a dataset with articles that were classified in 2 levels and we had to create a classifier that predictes both Level 1 and Level 2 classes.